{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis.app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agents and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agents' performance, if they select actions at random with each time step.  A window should pop up that allows you to observe the agents.\n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agents are able to use their experiences to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 6):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import argparse\n",
    "from collections import deque\n",
    "from ddpg_agent import Agent\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis.app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        , -6.65278625, -1.5       , -0.        ,  0.        ,\n",
       "         6.83172083,  6.        , -0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        , -6.4669857 , -1.5       ,  0.        ,  0.        ,\n",
       "        -6.83172083,  6.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'print_every' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1e17ce03fb63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mscores_deque\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeque\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprint_every\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi_episode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_episodes\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'print_every' is not defined"
     ]
    }
   ],
   "source": [
    "agent_1 = Agent(state_size=state_size, action_size=action_size, random_seed=2)\n",
    "\n",
    "\n",
    "scores_deque = deque(maxlen=print_every)\n",
    "scores = []\n",
    "for i_episode in range(1, n_episodes + 1):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "    agent.reset()\n",
    "    score = np.zeros(num_agents)\n",
    "    for t in range(max_t):\n",
    "        actions = agent.act(states, True)\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states, rewards, dones = env_info.vector_observations, env_info.rewards, env_info.local_done\n",
    "        # print(next_states.shape)\n",
    "        for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):\n",
    "            # print(done)\n",
    "            agent.step(state, action, reward, next_state, done, t)\n",
    "        states = next_states\n",
    "        score += rewards\n",
    "        if np.any(done):  # exit loop if episode finished\n",
    "            break\n",
    "    scores_deque.append(np.mean(score))\n",
    "    scores.append(np.mean(score))\n",
    "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)), end=\"\")\n",
    "    if i_episode % print_every == 0:\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "        torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env_info.vector_observations   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.expand_dims(states[0],0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_1 = Agent(state_size=state_size, action_size=action_size, random_seed=2)\n",
    "agent_2 = Agent(state_size=state_size, action_size=action_size, random_seed=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_2.memory = agent_1.memory\n",
    "agent_2.actor_local=agent_1.actor_local\n",
    "agent_2.actor_target=agent_1.actor_target\n",
    "agent_2.critic_local=agent_1.critic_local\n",
    "agent_2.critic_target=agent_1.critic_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = agent_1.act(np.expand_dims(states[0],0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.16014728, -0.16937067]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2 = agent_1.act(np.expand_dims(states[1],0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.16014728, -0.16937067],\n",
       "       [ 0.17613663, -0.14342788]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((a1, a2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (max over agents) from episode 9: 0.00000\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/d065820/Projects/RL_nanodegree/deep-reinforcement-learning/p3_collab-compet/ddpg_agent.py:108: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(self.critic_local.parameters(), 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (max over agents) from episode 100: 0.00000\n",
      " Episode 100\tAverage Score: 0.00\n",
      "Score (max over agents) from episode 200: 0.00000\n",
      " Episode 200\tAverage Score: 0.00\n",
      "Score (max over agents) from episode 300: 0.00000\n",
      " Episode 300\tAverage Score: 0.00\n",
      "Score (max over agents) from episode 400: 0.00000\n",
      " Episode 400\tAverage Score: 0.04\n",
      "Score (max over agents) from episode 500: 0.09000\n",
      " Episode 500\tAverage Score: 0.02\n",
      "Score (max over agents) from episode 600: 0.10000\n",
      " Episode 600\tAverage Score: 0.04\n",
      "Score (max over agents) from episode 700: 0.10000\n",
      " Episode 700\tAverage Score: 0.10\n",
      "Score (max over agents) from episode 793: 2.60000\n",
      " Episode 793\tAverage Score: 0.51\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 794: 0.70000\n",
      " Episode 794\tAverage Score: 0.52\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 795: 0.20000\n",
      " Episode 795\tAverage Score: 0.52\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 796: 0.20000\n",
      " Episode 796\tAverage Score: 0.52\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 797: 0.10000\n",
      " Episode 797\tAverage Score: 0.52\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 798: 0.10000\n",
      " Episode 798\tAverage Score: 0.52\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 799: 2.60000\n",
      " Episode 799\tAverage Score: 0.54\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 800: 1.00000\n",
      " Episode 800\tAverage Score: 0.55\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 801: 0.19000\n",
      " Episode 801\tAverage Score: 0.55\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 802: 0.10000\n",
      " Episode 802\tAverage Score: 0.55\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 803: 0.80000\n",
      " Episode 803\tAverage Score: 0.56\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 804: 0.20000\n",
      " Episode 804\tAverage Score: 0.56\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 805: 0.30000\n",
      " Episode 805\tAverage Score: 0.56\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 806: 0.10000\n",
      " Episode 806\tAverage Score: 0.56\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 807: 1.50000\n",
      " Episode 807\tAverage Score: 0.57\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 808: 0.19000\n",
      " Episode 808\tAverage Score: 0.57\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 809: 0.20000\n",
      " Episode 809\tAverage Score: 0.57\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 810: 0.20000\n",
      " Episode 810\tAverage Score: 0.57\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 811: 0.10000\n",
      " Episode 811\tAverage Score: 0.57\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 812: 0.20000\n",
      " Episode 812\tAverage Score: 0.57\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 813: 2.30000\n",
      " Episode 813\tAverage Score: 0.60\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 814: 0.10000\n",
      " Episode 814\tAverage Score: 0.60\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 815: 0.20000\n",
      " Episode 815\tAverage Score: 0.60\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 816: 0.20000\n",
      " Episode 816\tAverage Score: 0.60\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 817: 0.20000\n",
      " Episode 817\tAverage Score: 0.60\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 818: 0.10000\n",
      " Episode 818\tAverage Score: 0.60\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 819: 0.10000\n",
      " Episode 819\tAverage Score: 0.59\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 820: 0.09000\n",
      " Episode 820\tAverage Score: 0.59\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 821: 0.20000\n",
      " Episode 821\tAverage Score: 0.59\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 822: 0.10000\n",
      " Episode 822\tAverage Score: 0.59\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 823: 0.19000\n",
      " Episode 823\tAverage Score: 0.59\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 824: 0.20000\n",
      " Episode 824\tAverage Score: 0.59\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 825: 0.09000\n",
      " Episode 825\tAverage Score: 0.59\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 826: 0.19000\n",
      " Episode 826\tAverage Score: 0.59\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 827: 0.10000\n",
      " Episode 827\tAverage Score: 0.59\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 828: 0.10000\n",
      " Episode 828\tAverage Score: 0.59\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 829: 0.10000\n",
      " Episode 829\tAverage Score: 0.59\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 830: 0.10000\n",
      " Episode 830\tAverage Score: 0.59\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 831: 0.10000\n",
      " Episode 831\tAverage Score: 0.58\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 832: 0.09000\n",
      " Episode 832\tAverage Score: 0.58\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 833: 0.10000\n",
      " Episode 833\tAverage Score: 0.57\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 834: 0.20000\n",
      " Episode 834\tAverage Score: 0.57\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 835: 0.20000\n",
      " Episode 835\tAverage Score: 0.57\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 836: 0.10000\n",
      " Episode 836\tAverage Score: 0.57\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 837: 0.20000\n",
      " Episode 837\tAverage Score: 0.57\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 838: 0.10000\n",
      " Episode 838\tAverage Score: 0.55\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 839: 0.09000\n",
      " Episode 839\tAverage Score: 0.55\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 840: 0.10000\n",
      " Episode 840\tAverage Score: 0.54\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 841: 0.10000\n",
      " Episode 841\tAverage Score: 0.54\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 842: 0.09000\n",
      " Episode 842\tAverage Score: 0.53\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 843: 0.10000\n",
      " Episode 843\tAverage Score: 0.52\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 844: 0.40000\n",
      " Episode 844\tAverage Score: 0.51\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 845: 0.10000\n",
      " Episode 845\tAverage Score: 0.51\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 846: 0.10000\n",
      " Episode 846\tAverage Score: 0.51\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 847: 2.60000\n",
      " Episode 847\tAverage Score: 0.54\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 848: 2.60000\n",
      " Episode 848\tAverage Score: 0.54\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 849: 2.60000\n",
      " Episode 849\tAverage Score: 0.54\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 850: 0.70000\n",
      " Episode 850\tAverage Score: 0.53\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 851: 0.20000\n",
      " Episode 851\tAverage Score: 0.52\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 863: 2.50000\n",
      " Episode 863\tAverage Score: 0.50\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 866: 2.60000\n",
      " Episode 866\tAverage Score: 0.52\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 868: 2.60000\n",
      " Episode 868\tAverage Score: 0.52\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 869: 0.90000\n",
      " Episode 869\tAverage Score: 0.53\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 870: 1.50000\n",
      " Episode 870\tAverage Score: 0.54\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 871: 1.80000\n",
      " Episode 871\tAverage Score: 0.55\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 872: 0.09000\n",
      " Episode 872\tAverage Score: 0.54\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 873: 0.10000\n",
      " Episode 873\tAverage Score: 0.52\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 874: 0.10000\n",
      " Episode 874\tAverage Score: 0.52\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 875: 0.20000\n",
      " Episode 875\tAverage Score: 0.52\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 876: 0.10000\n",
      " Episode 876\tAverage Score: 0.52\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 877: 0.10000\n",
      " Episode 877\tAverage Score: 0.52\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 878: 0.10000\n",
      " Episode 878\tAverage Score: 0.52\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 879: 0.10000\n",
      " Episode 879\tAverage Score: 0.52\n",
      "\n",
      " Environment solved!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (max over agents) from episode 880: 0.20000\n",
      " Episode 880\tAverage Score: 0.51\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 881: 0.09000\n",
      " Episode 881\tAverage Score: 0.51\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 882: 0.20000\n",
      " Episode 882\tAverage Score: 0.51\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 883: 0.20000\n",
      " Episode 883\tAverage Score: 0.51\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 884: 0.20000\n",
      " Episode 884\tAverage Score: 0.51\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 885: 0.30000\n",
      " Episode 885\tAverage Score: 0.52\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 886: 0.10000\n",
      " Episode 886\tAverage Score: 0.51\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 887: 0.20000\n",
      " Episode 887\tAverage Score: 0.52\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 888: 2.60000\n",
      " Episode 888\tAverage Score: 0.54\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 889: 2.60000\n",
      " Episode 889\tAverage Score: 0.57\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 890: 0.20000\n",
      " Episode 890\tAverage Score: 0.56\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 891: 0.09000\n",
      " Episode 891\tAverage Score: 0.56\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 892: 0.09000\n",
      " Episode 892\tAverage Score: 0.53\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 893: 0.09000\n",
      " Episode 893\tAverage Score: 0.51\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 894: 0.09000\n",
      " Episode 894\tAverage Score: 0.50\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 895: 0.10000\n",
      " Episode 895\tAverage Score: 0.50\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 896: 0.10000\n",
      " Episode 896\tAverage Score: 0.50\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 897: 0.10000\n",
      " Episode 897\tAverage Score: 0.50\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 898: 0.09000\n",
      " Episode 898\tAverage Score: 0.50\n",
      "\n",
      " Environment solved!\n",
      "Score (max over agents) from episode 900: 0.09000\n",
      " Episode 900\tAverage Score: 0.47\n",
      "Score (max over agents) from episode 1000: 0.19000\n",
      " Episode 1000\tAverage Score: 0.23\n",
      "Score (max over agents) from episode 1022: 0.10000\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3ed319c0822a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_observations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m#for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0magent_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0magent_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/RL_nanodegree/deep-reinforcement-learning/p3_collab-compet/ddpg_agent.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, reward, next_state, done, t)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_UPDATES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_noise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/RL_nanodegree/deep-reinforcement-learning/p3_collab-compet/ddpg_agent.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, experiences, gamma)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mactor_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# ----------------------- update target networks ----------------------- #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/drlnd/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_episodes=5000\n",
    "t_max = 1000\n",
    "print_every=100\n",
    "maxlen=100\n",
    "\n",
    "score = []\n",
    "ev_score = []\n",
    "scores_deque = deque(maxlen=maxlen)\n",
    "for i_episode in range(1, n_episodes + 1):                                    # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    agent_1.reset()\n",
    "    agent_2.reset()   \n",
    "    for t in range(t_max):     \n",
    "        actions_1 = agent_1.act(np.expand_dims(states[0],0), True)\n",
    "        actions_2 = agent_2.act(np.expand_dims(states[1],0), True)\n",
    "        #actions_1 = np.clip(actions_1, -1, 1)             # all actions between -1 and 1\n",
    "        actions = np.concatenate((actions_1, actions_2))\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        \n",
    "        next_states, rewards, dones = env_info.vector_observations, env_info.rewards, env_info.local_done\n",
    "        #for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):\n",
    "        agent_1.step(np.expand_dims(states[0],0), actions_1, rewards[0], np.expand_dims(next_states[0],0), dones[0], t)\n",
    "        agent_2.step(np.expand_dims(states[1],0), actions_2, rewards[1], np.expand_dims(next_states[1],0), dones[1], t)\n",
    "        \n",
    "        scores += rewards                                  # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "        \n",
    "    score.append(np.max(scores))\n",
    "    ev_score.append(np.mean(scores_deque))\n",
    "    scores_deque.append(np.max(scores))\n",
    "    print('Score (max over agents) from episode {}: {:.5f}'.format(i_episode, np.max(scores)), end='\\r')\n",
    "    if i_episode % print_every == 0 or np.mean(scores_deque)>0.5:\n",
    "        print('\\n Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        if np.mean(scores_deque)>0.5:\n",
    "            print(\"\\n Environment solved!\")\n",
    "            torch.save(agent_1.actor_local.state_dict(), 'checkpoint_actor_1.pth')\n",
    "            torch.save(agent_2.actor_local.state_dict(), 'checkpoint_actor_2.pth')\n",
    "            torch.save(agent_1.critic_local.state_dict(), 'checkpoint_critic_1.pth')\n",
    "            torch.save(agent_2.critic_local.state_dict(), 'checkpoint_critic_2.pth')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_2.actor_local.load_state_dict(torch.load('checkpoint_actor_2.pth'))\n",
    "agent_1.actor_local.load_state_dict(torch.load('checkpoint_actor_1.pth'))\n",
    "agent_1.critic_local.load_state_dict(torch.load('checkpoint_critic_1.pth'))\n",
    "agent_2.critic_local.load_state_dict(torch.load('checkpoint_critic_2.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (max over agents) from episode 4: 2.60000\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "INFO:root:\n",
      "Unfortunately, your original traceback can not be constructed.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/d065820/anaconda2/envs/drlnd/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-8-b681fde24120>\", line 18, in <module>\n",
      "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
      "  File \"/Users/d065820/anaconda2/envs/drlnd/lib/python3.6/site-packages/unityagents/environment.py\", line 369, in step\n",
      "    self._generate_step_input(vector_action, memory, text_action)\n",
      "  File \"/Users/d065820/anaconda2/envs/drlnd/lib/python3.6/site-packages/unityagents/rpc_communicator.py\", line 78, in exchange\n",
      "    output = self.unity_to_external.parent_conn.recv()\n",
      "  File \"/Users/d065820/anaconda2/envs/drlnd/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Users/d065820/anaconda2/envs/drlnd/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/d065820/anaconda2/envs/drlnd/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/d065820/anaconda2/envs/drlnd/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2018, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/d065820/anaconda2/envs/drlnd/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/d065820/anaconda2/envs/drlnd/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/d065820/anaconda2/envs/drlnd/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Users/d065820/anaconda2/envs/drlnd/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Users/d065820/anaconda2/envs/drlnd/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Users/d065820/anaconda2/envs/drlnd/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Users/d065820/anaconda2/envs/drlnd/lib/python3.6/inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/Users/d065820/anaconda2/envs/drlnd/lib/python3.6/inspect.py\", line 709, in getabsfile\n",
      "    return os.path.normcase(os.path.abspath(_filename))\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "n_episodes=5000\n",
    "t_max = 1000\n",
    "print_every=100\n",
    "maxlen=100\n",
    "\n",
    "scores_deque = deque(maxlen=maxlen)\n",
    "for i_episode in range(1, n_episodes + 1):                                    # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    agent_1.reset()\n",
    "    agent_2.reset()   \n",
    "    for t in range(t_max):     \n",
    "        actions_1 = agent_1.act(np.expand_dims(states[0],0), True)\n",
    "        actions_2 = agent_2.act(np.expand_dims(states[1],0), True)\n",
    "        #actions_1 = np.clip(actions_1, -1, 1)             # all actions between -1 and 1\n",
    "        actions = np.concatenate((actions_1, actions_2))\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states, rewards, dones = env_info.vector_observations, env_info.rewards, env_info.local_done\n",
    "        scores += rewards                                  # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "            \n",
    "    scores_deque.append(np.max(scores))\n",
    "    print('Score (max over agents) from episode {}: {:.5f}'.format(i_episode, np.max(scores)), end='\\r')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):      \n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
